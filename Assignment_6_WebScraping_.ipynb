{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/syed-irtiza7/SyedIrtiza_DTSC3020_Fall2025/blob/main/Assignment_6_WebScraping_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_de5Eq4u-tR"
      },
      "source": [
        "# Assignment 6 (4 points) — Web Scraping\n",
        "\n",
        "In this assignment you will complete **two questions**. The **deadline is posted on Canvas**.\n"
      ],
      "id": "H_de5Eq4u-tR"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PHwamZMu-tX"
      },
      "source": [
        "## Assignment Guide (Read Me First)\n",
        "\n",
        "- This notebook provides an **Install Required Libraries** cell and a **Common Imports & Polite Headers** cell. Run them first.\n",
        "- Each question includes a **skeleton**. The skeleton is **not** a solution; it is a lightweight scaffold you may reuse.\n",
        "- Under each skeleton you will find a **“Write your answer here”** code cell. Implement your scraping, cleaning, and saving logic there.\n",
        "- When your code is complete, run the **Runner** cell to print a Top‑15 preview and save the CSV.\n",
        "- Expected outputs:\n",
        "  - **Q1:** `data_q1.csv` + Top‑15 sorted by the specified numeric column.\n",
        "  - **Q2:** `data_q2.csv` + Top‑15 sorted by `points`.\n"
      ],
      "id": "4PHwamZMu-tX"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "I7DLq9nEu-tZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73409c14-ce43-4371-8122-d212fbc47a29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dependencies installed.\n"
          ]
        }
      ],
      "source": [
        "# 1) Install Required Libraries\n",
        "!pip -q install requests beautifulsoup4 lxml pandas\n",
        "print(\"Dependencies installed.\")\n"
      ],
      "id": "I7DLq9nEu-tZ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ug_A9RuPu-tb"
      },
      "source": [
        "### 2) Common Imports & Polite Headers"
      ],
      "id": "ug_A9RuPu-tb"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Ov8pXh65u-tc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bb7c474-1d23-4d07-a75a-3aa16a84f685"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Common helpers loaded.\n"
          ]
        }
      ],
      "source": [
        "# Common Imports & Polite Headers\n",
        "import re, sys, pandas as pd, requests\n",
        "from bs4 import BeautifulSoup\n",
        "HEADERS = {\"User-Agent\": (\n",
        "    \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 \"\n",
        "    \"(KHTML, like Gecko) Chrome/122.0 Safari/537.36\")}\n",
        "def fetch_html(url: str, timeout: int = 20) -> str:\n",
        "    r = requests.get(url, headers=HEADERS, timeout=timeout)\n",
        "    r.raise_for_status()\n",
        "    return r.text\n",
        "def flatten_headers(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    if isinstance(df.columns, pd.MultiIndex):\n",
        "        df.columns = [\" \".join([str(x) for x in tup if str(x)!=\"nan\"]).strip()\n",
        "                      for tup in df.columns.values]\n",
        "    else:\n",
        "        df.columns = [str(c).strip() for c in df.columns]\n",
        "    return df\n",
        "print(\"Common helpers loaded.\")\n"
      ],
      "id": "Ov8pXh65u-tc"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "km0GO7zzu-td"
      },
      "source": [
        "## Question 1 — IBAN Country Codes (table)\n",
        "**URL:** https://www.iban.com/country-codes  \n",
        "**Extract at least:** `Country`, `Alpha-2`, `Alpha-3`, `Numeric` (≥4 cols; you may add more)  \n",
        "**Clean:** trim spaces; `Alpha-2/Alpha-3` → **UPPERCASE**; `Numeric` → **int** (nullable OK)  \n",
        "**Output:** write **`data_q1.csv`** and **print a Top-15** sorted by `Numeric` (desc, no charts)  \n",
        "**Deliverables:** notebook + `data_q1.csv` + short `README.md` (URL, steps, 1 limitation)\n",
        "\n",
        "**Tip:** You can use `pandas.read_html(html)` to read tables and then pick one with ≥3 columns.\n"
      ],
      "id": "km0GO7zzu-td"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "q1_skeleton"
      },
      "outputs": [],
      "source": [
        "# --- Q1 Skeleton (fill the TODOs) ---\n",
        "def q1_read_table(html: str) -> pd.DataFrame:\n",
        "    \"\"\"Return the first table with >= 3 columns from the HTML.\n",
        "    TODO: implement with pd.read_html(html), pick a reasonable table, then flatten headers.\n",
        "    \"\"\"\n",
        "    raise NotImplementedError(\"TODO: implement q1_read_table\")\n",
        "\n",
        "def q1_clean(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Clean columns: strip, UPPER Alpha-2/Alpha-3, cast Numeric to int (nullable), drop invalids.\n",
        "    TODO: implement cleaning steps.\n",
        "    \"\"\"\n",
        "    raise NotImplementedError(\"TODO: implement q1_clean\")\n",
        "\n",
        "def q1_sort_top(df: pd.DataFrame, top: int = 15) -> pd.DataFrame:\n",
        "    \"\"\"Sort descending by Numeric and return Top-N.\n",
        "    TODO: implement.\n",
        "    \"\"\"\n",
        "    raise NotImplementedError(\"TODO: implement q1_sort_top\")\n"
      ],
      "id": "q1_skeleton"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "q1_skeleton_answer",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bda48f05-0135-4ab5-bb52-3d1b1e979b32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Q1 - IBAN Country Codes scraping...\n",
            "✓ HTML content fetched successfully\n",
            "✓ Raw table extracted with 249 rows and 4 columns\n",
            "  Columns found: ['Country', 'Alpha-2 code', 'Alpha-3 code', 'Numeric']\n",
            "✓ Data cleaned: 249 valid rows remaining\n",
            "✓ Data saved to 'data_q1.csv'\n",
            "\n",
            "======================================================================\n",
            "Q1 RESULTS - Top 15 Countries by Numeric Code (Descending)\n",
            "======================================================================\n",
            "                                                   Country Alpha-2 Alpha-3  Numeric\n",
            "                                                    Zambia      ZM     ZMB      894\n",
            "                                                     Yemen      YE     YEM      887\n",
            "                                                     Samoa      WS     WSM      882\n",
            "                                         Wallis and Futuna      WF     WLF      876\n",
            "                        Venezuela (Bolivarian Republic of)      VE     VEN      862\n",
            "                                                Uzbekistan      UZ     UZB      860\n",
            "                                                   Uruguay      UY     URY      858\n",
            "                                              Burkina Faso      BF     BFA      854\n",
            "                                     Virgin Islands (U.S.)      VI     VIR      850\n",
            "                            United States of America (the)      US     USA      840\n",
            "                              Tanzania, United Republic of      TZ     TZA      834\n",
            "                                               Isle of Man      IM     IMN      833\n",
            "                                                    Jersey      JE     JEY      832\n",
            "                                                  Guernsey      GG     GGY      831\n",
            "United Kingdom of Great Britain and Northern Ireland (the)      GB     GBR      826\n",
            "\n",
            "Total countries in dataset: 249\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1330224901.py:6: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
            "  tables = pd.read_html(html)\n"
          ]
        }
      ],
      "source": [
        "# Q1 — Write your answer here\n",
        "\n",
        "def q1_read_table(html: str) -> pd.DataFrame:\n",
        "    \"\"\"Return the first table with >= 3 columns from the HTML.\"\"\"\n",
        "    # Read all tables from HTML\n",
        "    tables = pd.read_html(html)\n",
        "\n",
        "    # Find the first table with at least 3 columns\n",
        "    for table in tables:\n",
        "        if len(table.columns) >= 3:\n",
        "            # Flatten multi-level headers if they exist\n",
        "            df = flatten_headers(table)\n",
        "            return df\n",
        "\n",
        "    raise ValueError(\"No table with 3 or more columns found\")\n",
        "\n",
        "def q1_clean(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Clean columns: strip, UPPER Alpha-2/Alpha-3, cast Numeric to int (nullable), drop invalids.\"\"\"\n",
        "    # Make a copy to avoid modifying the original\n",
        "    df_clean = df.copy()\n",
        "\n",
        "    # Clean column names - strip whitespace\n",
        "    df_clean.columns = [col.strip() for col in df_clean.columns]\n",
        "\n",
        "    # Identify the correct columns (case-insensitive matching)\n",
        "    column_mapping = {}\n",
        "    for col in df_clean.columns:\n",
        "        col_lower = col.lower()\n",
        "        if 'country' in col_lower:\n",
        "            column_mapping['Country'] = col\n",
        "        elif 'alpha-2' in col_lower or 'alpha2' in col_lower.replace('-', ''):\n",
        "            column_mapping['Alpha-2'] = col\n",
        "        elif 'alpha-3' in col_lower or 'alpha3' in col_lower.replace('-', ''):\n",
        "            column_mapping['Alpha-3'] = col\n",
        "        elif 'numeric' in col_lower or 'code' in col_lower:\n",
        "            column_mapping['Numeric'] = col\n",
        "\n",
        "    # Select only the columns we need\n",
        "    required_cols = ['Country', 'Alpha-2', 'Alpha-3', 'Numeric']\n",
        "    available_cols = [column_mapping.get(col) for col in required_cols if column_mapping.get(col)]\n",
        "\n",
        "    if len(available_cols) < 4:\n",
        "        raise ValueError(f\"Could not find all required columns. Found: {available_cols}\")\n",
        "\n",
        "    df_clean = df_clean[available_cols].copy()\n",
        "    df_clean.columns = required_cols\n",
        "\n",
        "    # Clean data: strip whitespace from string columns\n",
        "    for col in ['Country', 'Alpha-2', 'Alpha-3']:\n",
        "        df_clean[col] = df_clean[col].astype(str).str.strip()\n",
        "\n",
        "    # Convert Alpha-2 and Alpha-3 to uppercase\n",
        "    df_clean['Alpha-2'] = df_clean['Alpha-2'].str.upper()\n",
        "    df_clean['Alpha-3'] = df_clean['Alpha-3'].str.upper()\n",
        "\n",
        "    # Convert Numeric to integer (nullable)\n",
        "    df_clean['Numeric'] = pd.to_numeric(df_clean['Numeric'], errors='coerce').astype('Int64')\n",
        "\n",
        "    # Drop rows with missing critical values\n",
        "    df_clean = df_clean.dropna(subset=['Country', 'Alpha-2', 'Alpha-3', 'Numeric'])\n",
        "\n",
        "    # Remove any rows where country name is obviously invalid\n",
        "    df_clean = df_clean[~df_clean['Country'].isin(['', 'Country', 'Country Name'])]\n",
        "\n",
        "    return df_clean\n",
        "\n",
        "def q1_sort_top(df: pd.DataFrame, top: int = 15) -> pd.DataFrame:\n",
        "    \"\"\"Sort descending by Numeric and return Top-N.\"\"\"\n",
        "    return df.sort_values('Numeric', ascending=False).head(top)\n",
        "\n",
        "# Execute Q1 pipeline\n",
        "try:\n",
        "    print(\"Starting Q1 - IBAN Country Codes scraping...\")\n",
        "\n",
        "    # Fetch HTML content\n",
        "    url = \"https://www.iban.com/country-codes\"\n",
        "    html_content = fetch_html(url)\n",
        "    print(\"✓ HTML content fetched successfully\")\n",
        "\n",
        "    # Read and extract table\n",
        "    raw_df = q1_read_table(html_content)\n",
        "    print(f\"✓ Raw table extracted with {len(raw_df)} rows and {len(raw_df.columns)} columns\")\n",
        "    print(f\"  Columns found: {list(raw_df.columns)}\")\n",
        "\n",
        "    # Clean the data\n",
        "    cleaned_df = q1_clean(raw_df)\n",
        "    print(f\"✓ Data cleaned: {len(cleaned_df)} valid rows remaining\")\n",
        "\n",
        "    # Get top 15 sorted by Numeric (descending)\n",
        "    top_15 = q1_sort_top(cleaned_df, 15)\n",
        "\n",
        "    # Save to CSV\n",
        "    cleaned_df.to_csv('data_q1.csv', index=False)\n",
        "    print(\"✓ Data saved to 'data_q1.csv'\")\n",
        "\n",
        "    # Display results\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"Q1 RESULTS - Top 15 Countries by Numeric Code (Descending)\")\n",
        "    print(\"=\"*70)\n",
        "    print(top_15.to_string(index=False))\n",
        "    print(f\"\\nTotal countries in dataset: {len(cleaned_df)}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"❌ Error in Q1: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "\n",
        "\n"
      ],
      "id": "q1_skeleton_answer"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmefu--_u-tg"
      },
      "source": [
        "## Question 2 — Hacker News (front page)\n",
        "**URL:** https://news.ycombinator.com/  \n",
        "**Extract at least:** `rank`, `title`, `link`, `points`, `comments` (user optional)  \n",
        "**Clean:** cast `points`/`comments`/`rank` → **int** (non-digits → 0), fill missing text fields  \n",
        "**Output:** write **`data_q2.csv`** and **print a Top-15** sorted by `points` (desc, no charts)  \n",
        "**Tip:** Each story is a `.athing` row; details (points/comments/user) are in the next `<tr>` with `.subtext`.\n"
      ],
      "id": "rmefu--_u-tg"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "q2_skeleton"
      },
      "outputs": [],
      "source": [
        "# --- Q2 Skeleton (fill the TODOs) ---\n",
        "def q2_parse_items(html: str) -> pd.DataFrame:\n",
        "    \"\"\"Parse front page items into DataFrame columns:\n",
        "       rank, title, link, points, comments, user (optional).\n",
        "    TODO: implement with BeautifulSoup on '.athing' and its sibling '.subtext'.\n",
        "    \"\"\"\n",
        "    raise NotImplementedError(\"TODO: implement q2_parse_items\")\n",
        "\n",
        "def q2_clean(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Clean numeric fields and fill missing values.\n",
        "    TODO: cast points/comments/rank to int (non-digits -> 0). Fill text fields.\n",
        "    \"\"\"\n",
        "    raise NotImplementedError(\"TODO: implement q2_clean\")\n",
        "\n",
        "def q2_sort_top(df: pd.DataFrame, top: int = 15) -> pd.DataFrame:\n",
        "    \"\"\"Sort by points desc and return Top-N. TODO: implement.\"\"\"\n",
        "    raise NotImplementedError(\"TODO: implement q2_sort_top\")\n"
      ],
      "id": "q2_skeleton"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "q2_skeleton_answer",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e10ff290-c3d3-4ab8-a8b7-ac8f89429e9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Q2 - Hacker News Front Page scraping...\n",
            "✓ HTML content fetched successfully\n",
            "✓ Parsed 30 stories from Hacker News front page\n",
            "✓ Data cleaned: 30 valid stories\n",
            "✓ Data saved to 'data_q2.csv'\n",
            "\n",
            "==========================================================================================\n",
            "Q2 RESULTS - Top 15 Hacker News Stories by Points (Descending)\n",
            "==========================================================================================\n",
            " rank title  points  comments         user\n",
            "   13           502       184  WaitWaitWha\n",
            "    9           311       211        art-w\n",
            "   28           282        47    kirschner\n",
            "   30           275       152    granzymes\n",
            "    1           234       118    vitalnodo\n",
            "    6           232        86  birdculture\n",
            "    8           202        77 joemasilotti\n",
            "    5           185        33    zachlatta\n",
            "    3           165        68    lalitkale\n",
            "    2           158        38  yehiaabdelm\n",
            "   29           124        59      bookmtn\n",
            "   22           123        27     azhenley\n",
            "   17           105        23  bramadityaw\n",
            "   18            87         1     defmarco\n",
            "    4            86        39    holysoles\n",
            "\n",
            "Total stories scraped: 30\n"
          ]
        }
      ],
      "source": [
        "# Q2 — Write your answer here\n",
        "\n",
        "def q2_parse_items(html: str) -> pd.DataFrame:\n",
        "    \"\"\"Parse front page items into DataFrame columns:\n",
        "       rank, title, link, points, comments, user (optional).\n",
        "    \"\"\"\n",
        "    soup = BeautifulSoup(html, 'html.parser')\n",
        "\n",
        "    items = []\n",
        "\n",
        "    # Find all story rows with class 'athing'\n",
        "    story_rows = soup.find_all('tr', class_='athing')\n",
        "\n",
        "    for story in story_rows:\n",
        "        item_data = {}\n",
        "\n",
        "        # Extract rank\n",
        "        rank_span = story.find('span', class_='rank')\n",
        "        item_data['rank'] = rank_span.text.strip().replace('.', '') if rank_span else ''\n",
        "\n",
        "        # Extract title and link\n",
        "        title_link = story.find('a', class_='titlelink')\n",
        "        item_data['title'] = title_link.text if title_link else ''\n",
        "        item_data['link'] = title_link.get('href', '') if title_link else ''\n",
        "\n",
        "        # Find the next sibling row with class 'subtext' for metadata\n",
        "        subtext_row = story.find_next_sibling('tr')\n",
        "        points = '0'\n",
        "        user = ''\n",
        "        comments = '0'\n",
        "\n",
        "        if subtext_row and subtext_row.find('td', class_='subtext'):\n",
        "            subtext = subtext_row.find('td', class_='subtext')\n",
        "\n",
        "            # Extract points\n",
        "            score_span = subtext.find('span', class_='score')\n",
        "            if score_span:\n",
        "                points = score_span.text.replace(' points', '').replace(' point', '')\n",
        "\n",
        "            # Extract user\n",
        "            user_link = subtext.find('a', class_='hnuser')\n",
        "            if user_link:\n",
        "                user = user_link.text\n",
        "\n",
        "            # Extract comments\n",
        "            all_links = subtext.find_all('a')\n",
        "            if len(all_links) >= 3:\n",
        "                comments_link = all_links[-1]  # Last link is usually comments\n",
        "                comments_text = comments_link.text\n",
        "                if 'comment' in comments_text.lower():\n",
        "                    # Extract number from text like \"42 comments\" or \"discuss\"\n",
        "                    if 'discuss' in comments_text.lower():\n",
        "                        comments = '0'\n",
        "                    else:\n",
        "                        comments = comments_text.split()[0]\n",
        "\n",
        "        item_data['points'] = points\n",
        "        item_data['user'] = user\n",
        "        item_data['comments'] = comments\n",
        "\n",
        "        items.append(item_data)\n",
        "\n",
        "    return pd.DataFrame(items)\n",
        "\n",
        "def q2_clean(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Clean numeric fields and fill missing values.\"\"\"\n",
        "    df_clean = df.copy()\n",
        "\n",
        "    # Clean rank - remove non-numeric characters and convert to int\n",
        "    df_clean['rank'] = df_clean['rank'].astype(str).str.replace(r'\\D', '', regex=True)\n",
        "    df_clean['rank'] = pd.to_numeric(df_clean['rank'], errors='coerce').fillna(0).astype(int)\n",
        "\n",
        "    # Clean points - extract numbers only\n",
        "    df_clean['points'] = df_clean['points'].astype(str).str.replace(r'\\D', '', regex=True)\n",
        "    df_clean['points'] = pd.to_numeric(df_clean['points'], errors='coerce').fillna(0).astype(int)\n",
        "\n",
        "    # Clean comments - handle special cases and extract numbers\n",
        "    df_clean['comments'] = df_clean['comments'].astype(str)\n",
        "\n",
        "    # Replace \"discuss\" with 0 and extract numbers from comments text\n",
        "    def clean_comment_text(text):\n",
        "        text = str(text).lower().strip()\n",
        "        if text in ['discuss', '']:\n",
        "            return '0'\n",
        "        # Extract first number found in the text\n",
        "        numbers = re.findall(r'\\d+', text)\n",
        "        return numbers[0] if numbers else '0'\n",
        "\n",
        "    df_clean['comments'] = df_clean['comments'].apply(clean_comment_text)\n",
        "    df_clean['comments'] = pd.to_numeric(df_clean['comments'], errors='coerce').fillna(0).astype(int)\n",
        "\n",
        "    # Fill missing text fields with empty strings\n",
        "    text_columns = ['title', 'link', 'user']\n",
        "    for col in text_columns:\n",
        "        df_clean[col] = df_clean[col].fillna('').astype(str)\n",
        "\n",
        "    # Strip whitespace from text fields\n",
        "    for col in text_columns:\n",
        "        df_clean[col] = df_clean[col].str.strip()\n",
        "\n",
        "    return df_clean\n",
        "\n",
        "def q2_sort_top(df: pd.DataFrame, top: int = 15) -> pd.DataFrame:\n",
        "    \"\"\"Sort by points desc and return Top-N.\"\"\"\n",
        "    return df.sort_values('points', ascending=False).head(top)\n",
        "\n",
        "# Execute Q2 pipeline\n",
        "try:\n",
        "    print(\"Starting Q2 - Hacker News Front Page scraping...\")\n",
        "\n",
        "    # Fetch HTML content\n",
        "    url = \"https://news.ycombinator.com/\"\n",
        "    html_content = fetch_html(url)\n",
        "    print(\"✓ HTML content fetched successfully\")\n",
        "\n",
        "    # Parse items from HTML\n",
        "    raw_df = q2_parse_items(html_content)\n",
        "    print(f\"✓ Parsed {len(raw_df)} stories from Hacker News front page\")\n",
        "\n",
        "    # Clean the data\n",
        "    cleaned_df = q2_clean(raw_df)\n",
        "    print(f\"✓ Data cleaned: {len(cleaned_df)} valid stories\")\n",
        "\n",
        "    # Get top 15 sorted by points (descending)\n",
        "    top_15 = q2_sort_top(cleaned_df, 15)\n",
        "\n",
        "    # Save to CSV\n",
        "    cleaned_df.to_csv('data_q2.csv', index=False)\n",
        "    print(\"✓ Data saved to 'data_q2.csv'\")\n",
        "\n",
        "    # Display results\n",
        "    print(\"\\n\" + \"=\"*90)\n",
        "    print(\"Q2 RESULTS - Top 15 Hacker News Stories by Points (Descending)\")\n",
        "    print(\"=\"*90)\n",
        "\n",
        "    # Format output for better readability\n",
        "    display_df = top_15[['rank', 'title', 'points', 'comments', 'user']].copy()\n",
        "\n",
        "    # Truncate long titles for better display\n",
        "    display_df['title'] = display_df['title'].apply(\n",
        "        lambda x: x[:70] + '...' if len(x) > 70 else x\n",
        "    )\n",
        "\n",
        "    print(display_df.to_string(index=False))\n",
        "    print(f\"\\nTotal stories scraped: {len(cleaned_df)}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"❌ Error in Q2: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "\n"
      ],
      "id": "q2_skeleton_answer"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}